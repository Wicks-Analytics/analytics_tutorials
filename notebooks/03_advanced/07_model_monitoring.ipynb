{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 07: Model Monitoring and Drift Detection\n",
    "In this tutorial, you'll learn:\n",
    "- How to detect data drift in production models\n",
    "- Using PSI (Population Stability Index) for feature drift\n",
    "- Monitoring model performance over time\n",
    "- Statistical tests for population comparison\n",
    "- Setting up automated monitoring workflows\n",
    "Scenario:\n",
    "Your fraud detection model has been in production for several months.\n",
    "You need to monitor whether the data distribution has changed and if\n",
    "the model performance is degrading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from analytics_store import model_validation, monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Loading baseline data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = project_root / \"data\" / \"fraud_predictions.csv\"\n",
    "\n",
    "if not data_path.exists():\n",
    "    print(f\"‚ùå Data file not found: {data_path}\")\n",
    "    print(\"Please run: python setup_database.py\")\n",
    "\n",
    "baseline_df = pl.read_csv(data_path)\n",
    "print(f\"‚úì Loaded {len(baseline_df)} baseline predictions\")\n",
    "\n",
    "# Split into baseline and current\n",
    "baseline = baseline_df.head(3000)\n",
    "current = baseline_df.tail(2000)\n",
    "\n",
    "print(f\"- Baseline period: {len(baseline)} samples\")\n",
    "print(f\"- Current period: {len(current)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Comparing score distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this example, compare baseline vs current from different data\n",
    "current_scores = current[\"model1_fraud_score\"].to_list()\n",
    "baseline_scores = baseline[\"model1_fraud_score\"].to_list()\n",
    "\n",
    "# Create combined dataframe for comparison\n",
    "comparison_df = pl.DataFrame(\n",
    "    {\n",
    "        \"baseline_scores\": baseline_scores[: min(len(baseline_scores), len(current_scores))],\n",
    "        \"current_scores\": current_scores[: min(len(baseline_scores), len(current_scores))],\n",
    "    }\n",
    ")\n",
    "\n",
    "result = monitoring.compare_populations(\n",
    "    comparison_df, column1=\"baseline_scores\", column2=\"current_scores\", alpha=0.05, test_type=\"auto\"\n",
    ")\n",
    "\n",
    "print(\"\\nPopulation Comparison Results:\")\n",
    "print(f\"- Test Type: {result.test_type}\")\n",
    "print(f\"- Test Statistic: {result.statistic:.4f}\")\n",
    "print(f\"- P-value: {result.p_value:.4f}\")\n",
    "print(f\"- Effect Size: {result.effect_size:.4f}\")\n",
    "print(f\"- Significant Difference: {result.is_significant}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Interpreting effect size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effect_size = abs(result.effect_size)\n",
    "\n",
    "if effect_size < 0.2:\n",
    "    interpretation = \"Negligible\"\n",
    "    action = \"No action needed\"\n",
    "elif effect_size < 0.5:\n",
    "    interpretation = \"Small\"\n",
    "    action = \"Monitor closely\"\n",
    "elif effect_size < 0.8:\n",
    "    interpretation = \"Medium\"\n",
    "    action = \"Investigate and consider retraining\"\n",
    "else:\n",
    "    interpretation = \"Large\"\n",
    "    action = \"Immediate action required - retrain model\"\n",
    "\n",
    "print(f\"\\nEffect Size: {effect_size:.4f} - {interpretation}\")\n",
    "print(f\"Recommended Action: {action}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Simulating data drift scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create drifted data\n",
    "# Note: generate_drifted_data is a placeholder function\n",
    "# In practice, you would load actual production data\n",
    "\n",
    "# For demonstration, add noise to simulate drift\n",
    "drifted_scores = baseline[\"model1_fraud_score\"] + np.random.normal(0, 0.1, len(baseline))\n",
    "drifted_df = baseline.with_columns([drifted_scores.alias(\"model1_fraud_score_drifted\")])\n",
    "\n",
    "# Compare baseline vs drifted\n",
    "drift_comparison = pl.DataFrame(\n",
    "    {\n",
    "        \"baseline\": baseline[\"model1_fraud_score\"].to_list()[:2000],\n",
    "        \"drifted\": drifted_df[\"model1_fraud_score_drifted\"].to_list()[:2000],\n",
    "    }\n",
    ")\n",
    "\n",
    "drift_result = monitoring.compare_populations(\n",
    "    drift_comparison, column1=\"baseline\", column2=\"drifted\", alpha=0.05, test_type=\"auto\"\n",
    ")\n",
    "\n",
    "print(\"\\nDrift Detection Results:\")\n",
    "print(f\"- P-value: {drift_result.p_value:.4f}\")\n",
    "print(f\"- Effect Size: {drift_result.effect_size:.4f}\")\n",
    "print(f\"- Drift Detected: {drift_result.is_significant}\")\n",
    "\n",
    "if drift_result.is_significant:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: Significant drift detected!\")\n",
    "    print(\"   Model may need retraining\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Monitoring performance over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate monthly performance\n",
    "months = [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\"]\n",
    "monthly_metrics = []\n",
    "\n",
    "print(\"\\nMonthly Performance Tracking:\")\n",
    "print(f\"{'Month':<8} {'AUC':<10} {'Top Decile Lift':<18} {'Change':<10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Split data into monthly chunks\n",
    "chunk_size = len(baseline_df) // 6\n",
    "\n",
    "for i, month in enumerate(months):\n",
    "    start_idx = i * chunk_size\n",
    "    end_idx = start_idx + chunk_size\n",
    "    month_data = baseline_df[start_idx:end_idx]\n",
    "\n",
    "    # Calculate metrics\n",
    "    roc_result = model_validation.calculate_roc_curve(\n",
    "        month_data, target_column=\"actual_fraud\", score_column=\"model1_fraud_score\"\n",
    "    )\n",
    "\n",
    "    lift_result = model_validation.calculate_lift_curve(\n",
    "        month_data, target_column=\"actual_fraud\", score_column=\"model1_fraud_score\", n_bins=10\n",
    "    )\n",
    "\n",
    "    monthly_metrics.append(\n",
    "        {\n",
    "            \"month\": month,\n",
    "            \"auc\": roc_result.auc_score,\n",
    "            \"top_decile_lift\": lift_result.score_lift_values[0],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Calculate change from previous month\n",
    "    if i > 0:\n",
    "        auc_change = roc_result.auc_score - monthly_metrics[i - 1][\"auc\"]\n",
    "        change_str = f\"{auc_change:+.4f}\"\n",
    "    else:\n",
    "        change_str = \"baseline\"\n",
    "\n",
    "    print(\n",
    "        f\"{month:<8} {roc_result.auc_score:<10.4f} \"\n",
    "        f\"{lift_result.score_lift_values[0]:<18.2f} {change_str:<10}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Setting up monitoring thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_auc = monthly_metrics[0][\"auc\"]\n",
    "\n",
    "print(f\"\\nBaseline AUC: {baseline_auc:.4f}\")\n",
    "print(\"\\nMonitoring Thresholds:\")\n",
    "print(f\"- Warning (5% drop): {baseline_auc * 0.95:.4f}\")\n",
    "print(f\"- Critical (10% drop): {baseline_auc * 0.90:.4f}\")\n",
    "\n",
    "# Check current performance\n",
    "current_auc = monthly_metrics[-1][\"auc\"]\n",
    "drop_pct = ((baseline_auc - current_auc) / baseline_auc) * 100\n",
    "\n",
    "print(f\"\\nCurrent AUC: {current_auc:.4f}\")\n",
    "print(f\"Performance Drop: {drop_pct:.1f}%\")\n",
    "\n",
    "if drop_pct >= 10:\n",
    "    print(\"üî¥ CRITICAL: Performance degradation detected!\")\n",
    "elif drop_pct >= 5:\n",
    "    print(\"üü° WARNING: Performance decline detected\")\n",
    "else:\n",
    "    print(\"üü¢ OK: Performance within acceptable range\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Analyzing feature distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare score distributions\n",
    "print(\"\\nScore Distribution Comparison:\")\n",
    "print(f\"{'Metric':<20} {'Baseline':<15} {'Current':<15} {'Change':<10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "baseline_stats = {\n",
    "    \"Mean\": baseline[\"model1_fraud_score\"].mean(),\n",
    "    \"Median\": baseline[\"model1_fraud_score\"].median(),\n",
    "    \"Std Dev\": baseline[\"model1_fraud_score\"].std(),\n",
    "    \"Min\": baseline[\"model1_fraud_score\"].min(),\n",
    "    \"Max\": baseline[\"model1_fraud_score\"].max(),\n",
    "}\n",
    "\n",
    "current_stats = {\n",
    "    \"Mean\": current[\"model1_fraud_score\"].mean(),\n",
    "    \"Median\": current[\"model1_fraud_score\"].median(),\n",
    "    \"Std Dev\": current[\"model1_fraud_score\"].std(),\n",
    "    \"Min\": current[\"model1_fraud_score\"].min(),\n",
    "    \"Max\": current[\"model1_fraud_score\"].max(),\n",
    "}\n",
    "\n",
    "for metric in baseline_stats.keys():\n",
    "    baseline_val = baseline_stats[metric]\n",
    "    current_val = current_stats[metric]\n",
    "    change = current_val - baseline_val\n",
    "    print(f\"{metric:<20} {baseline_val:<15.4f} {current_val:<15.4f} {change:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Generating monitoring report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"baseline_samples\": len(baseline),\n",
    "    \"current_samples\": len(current),\n",
    "    \"baseline_auc\": baseline_auc,\n",
    "    \"current_auc\": current_auc,\n",
    "    \"performance_drop_pct\": drop_pct,\n",
    "    \"drift_detected\": drift_result.is_significant,\n",
    "    \"drift_p_value\": drift_result.p_value,\n",
    "    \"drift_effect_size\": drift_result.effect_size,\n",
    "}\n",
    "\n",
    "report_df = pl.DataFrame([report])\n",
    "\n",
    "output_dir = project_root / \"outputs\"\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "report_df.write_csv(output_dir / \"07_monitoring_report.csv\")\n",
    "\n",
    "print(f\"‚úì Report saved to: {output_dir / '07_monitoring_report.csv'}\")\n",
    "print(\"\\nReport Summary:\")\n",
    "\n",
    "for key, value in report.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Automated monitoring workflow example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"\"\"\n",
    "def monitor_model_performance(baseline_df, current_df, thresholds):\n",
    "    '''\n",
    "    Automated monitoring function to run daily/weekly.\n",
    "    '''\n",
    "    # 1. Compare populations\n",
    "    result = monitoring.compare_populations(\n",
    "        baseline_df, 'score', 'score'\n",
    "    )\n",
    "\n",
    "    # 2. Calculate current metrics\n",
    "    current_auc = calculate_roc_curve(\n",
    "        current_df, 'actual', 'score'\n",
    "    ).auc_score\n",
    "\n",
    "    # 3. Check thresholds\n",
    "    alerts = []\n",
    "    if result.is_significant:\n",
    "        alerts.append('Data drift detected')\n",
    "\n",
    "    if current_auc < thresholds['critical']:\n",
    "        alerts.append('Critical performance drop')\n",
    "    elif current_auc < thresholds['warning']:\n",
    "        alerts.append('Performance warning')\n",
    "\n",
    "    # 4. Send alerts if needed\n",
    "    if alerts:\n",
    "        send_alert(alerts)\n",
    "\n",
    "    # 5. Log metrics\n",
    "    log_metrics(current_auc, result.p_value)\n",
    "\n",
    "    return alerts\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Step 10: Exercise\n",
    "print(\"\\nüéì EXERCISE: Build a Monitoring Dashboard\")\n",
    "print(\n",
    "    \"\"\"\n",
    "Create a monitoring script that:\n",
    "\n",
    "1. Loads baseline and current data from database\n",
    "2. Calculates multiple metrics (AUC, lift, precision, recall)\n",
    "3. Performs drift detection on scores\n",
    "4. Generates alerts based on thresholds\n",
    "5. Saves results to a monitoring table\n",
    "6. Creates visualization of metrics over time\n",
    "\n",
    "Bonus: Set up scheduled execution (e.g., daily cron job)\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"\\nKey Takeaways:\")\n",
    "print(\"1. Monitor both data drift and performance metrics\")\n",
    "print(\"2. Use statistical tests to detect significant changes\")\n",
    "print(\"3. Set up thresholds for automated alerting\")\n",
    "print(\"4. Track metrics over time to identify trends\")\n",
    "print(\"5. Effect size helps prioritize actions\")\n",
    "print(\"\\nNext: Tutorial 08 - Snowflake Integration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Try the exercise below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
