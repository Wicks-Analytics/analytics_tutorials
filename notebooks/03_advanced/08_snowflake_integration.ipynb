{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 08: Snowflake Integration",
    "In this tutorial, you'll learn:",
    "- How to connect to Snowflake data warehouse",
    "- Loading large datasets efficiently from Snowflake",
    "- Best practices for cloud data warehouse integration",
    "- Performing analytics on Snowflake data",
    "Scenario:",
    "Your insurance company stores data in Snowflake. You need to load data",
    "for analysis while minimizing data transfer and compute costs.",
    "Prerequisites:",
    "- Snowflake account with appropriate permissions",
    "- Environment variables or config file with credentials",
    "- snowflake-connector-python package installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl",
    "import sys",
    "from pathlib import Path",
    "import os",
    "",
    "# Add project root to path",
    "project_root = Path.cwd().parent.parent",
    "sys.path.insert(0, str(project_root))",
    "",
    "from analytics_store import model_validation, validation_plots",
    "from utils.database_helpers import get_snowflake_connection, load_from_snowflake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Checking Snowflake configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not check_snowflake_config():",
    "    print(\"\\nðŸ’¡ For this tutorial, you need:\")",
    "    print(\"   1. A Snowflake account\")",
    "    print(\"   2. Credentials set in environment variables\")",
    "    print(\"   3. Data loaded in Snowflake tables\")",
    "    print(\"\\nExample .env file:\")",
    "    print(\"\"\"",
    "    SNOWFLAKE_ACCOUNT=your_account.region",
    "    SNOWFLAKE_USER=your_username",
    "    SNOWFLAKE_PASSWORD=your_password",
    "    SNOWFLAKE_WAREHOUSE=your_warehouse",
    "    SNOWFLAKE_DATABASE=INSURANCE_DB",
    "    SNOWFLAKE_SCHEMA=PUBLIC",
    "    SNOWFLAKE_ROLE=ANALYST",
    "    \"\"\")",
    "    print(\"\\nSkipping Snowflake connection (demo mode)...\")",
    "    demo_mode = True",
    "else:",
    "    demo_mode = False",
    "    print(\"âœ“ Snowflake configuration found\")",
    "# Step 2: Connect to Snowflake",
    "if not demo_mode:",
    "    print(\"\\nðŸ”Œ Step 2: Connecting to Snowflake...\")",
    "    try:",
    "        conn_params = get_snowflake_connection()",
    "        print(f\"âœ“ Connecting to account: {conn_params['account']}\")",
    "        print(f\"âœ“ Using warehouse: {conn_params['warehouse']}\")",
    "        print(f\"âœ“ Database: {conn_params['database']}\")",
    "    except Exception as e:",
    "        print(f\"âŒ Connection error: {e}\")",
    "        demo_mode = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Efficient Data Loading Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nStrategy 1: Use SELECT with specific columns\")",
    "print(\"\"\"",
    "# Instead of SELECT *",
    "query = '''",
    "    SELECT",
    "        policy_id,",
    "        customer_age,",
    "        annual_premium,",
    "        policy_type",
    "    FROM INSURANCE_DB.PUBLIC.POLICIES",
    "    LIMIT 10000",
    "'''",
    "df = load_from_snowflake(query)",
    "\"\"\")",
    "print(\"\\nStrategy 2: Filter data in Snowflake (not in Python)\")",
    "print(\"\"\"",
    "# Push filtering to Snowflake",
    "query = '''",
    "    SELECT *",
    "    FROM INSURANCE_DB.PUBLIC.CLAIMS",
    "    WHERE claim_date >= DATEADD(month, -6, CURRENT_DATE())",
    "    AND claim_amount > 10000",
    "    AND state IN ('CA', 'NY', 'TX')",
    "'''",
    "df = load_from_snowflake(query)",
    "\"\"\")",
    "print(\"\\nStrategy 3: Use aggregations in Snowflake\")",
    "print(\"\"\"",
    "# Aggregate before loading",
    "query = '''",
    "    SELECT",
    "        policy_type,",
    "        state,",
    "        DATE_TRUNC('month', claim_date) as month,",
    "        COUNT(*) as claim_count,",
    "        AVG(claim_amount) as avg_claim,",
    "        SUM(CASE WHEN is_fraud = 1 THEN 1 ELSE 0 END) as fraud_count",
    "    FROM INSURANCE_DB.PUBLIC.CLAIMS",
    "    GROUP BY policy_type, state, month",
    "    ORDER BY month DESC",
    "'''",
    "df = load_from_snowflake(query)",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Sample Queries for Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nQuery 1: Load fraud predictions for model evaluation\")",
    "fraud_query = \"\"\"",
    "SELECT",
    "    claim_id,",
    "    actual_fraud,",
    "    model1_fraud_score,",
    "    model2_fraud_score,",
    "    claim_amount,",
    "    policy_type",
    "FROM INSURANCE_DB.PUBLIC.FRAUD_PREDICTIONS",
    "WHERE prediction_date >= DATEADD(day, -30, CURRENT_DATE())",
    "\"\"\"",
    "print(fraud_query)",
    "print(\"\\nQuery 2: Load premium predictions with features\")",
    "premium_query = \"\"\"",
    "SELECT",
    "    policy_id,",
    "    customer_age,",
    "    credit_score,",
    "    coverage_amount,",
    "    actual_premium,",
    "    model1_predicted_premium,",
    "    model2_predicted_premium",
    "FROM INSURANCE_DB.PUBLIC.PREMIUM_PREDICTIONS",
    "WHERE prediction_date = CURRENT_DATE()",
    "\"\"\"",
    "print(premium_query)",
    "print(\"\\nQuery 3: Complex join for comprehensive analysis\")",
    "complex_query = \"\"\"",
    "SELECT",
    "    c.claim_id,",
    "    c.claim_amount,",
    "    c.claim_type,",
    "    c.is_fraud,",
    "    c.fraud_score,",
    "    p.policy_type,",
    "    p.customer_age,",
    "    p.state,",
    "    p.annual_premium,",
    "    p.coverage_amount,",
    "    p.credit_score",
    "FROM INSURANCE_DB.PUBLIC.CLAIMS c",
    "INNER JOIN INSURANCE_DB.PUBLIC.POLICIES p",
    "    ON c.policy_id = p.policy_id",
    "WHERE c.claim_date >= DATEADD(year, -1, CURRENT_DATE())",
    "AND c.claim_status = 'Approved'",
    "\"\"\"",
    "print(complex_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Demo Analysis (using local data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load local data to simulate Snowflake data",
    "data_path = project_root / 'data' / 'fraud_predictions.csv'",
    "if data_path.exists():",
    "    print(\"Loading sample data (simulating Snowflake query)...\")",
    "    df = pl.read_csv(data_path)",
    "    # Perform analysis",
    "    print(f\"âœ“ Loaded {len(df)} records\")",
    "    # Calculate metrics",
    "    lift_result = model_validation.calculate_lift_curve(",
    "        df,",
    "        target_column='actual_fraud',",
    "        score_column='model1_fraud_score',",
    "        n_bins=10",
    "    )",
    "    print(f\"\\nModel Performance:\")",
    "    print(f\"- AUC Lift: {lift_result.auc_score_lift:.4f}\")",
    "    print(f\"- Top decile lift: {lift_result.score_lift_values[0]:.2f}x\")",
    "    # ROC analysis",
    "    roc_result = model_validation.calculate_roc_curve(",
    "        df,",
    "        target_column='actual_fraud',",
    "        score_column='model1_fraud_score'",
    "    )",
    "    print(f\"- AUC Score: {roc_result.auc_score:.4f}\")",
    "    print(f\"- Optimal threshold: {roc_result.optimal_threshold:.4f}\")",
    "else:",
    "    print(\"âš  Sample data not found. Run: python utils/data_generators.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Snowflake Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"\"\"âœ“ Use warehouse size appropriate for your queryâœ“ Filter and aggregate in Snowflake, not in Pythonâœ“ Use LIMIT for exploratory queriesâœ“ Consider result caching for repeated queriesâœ“ Use clustering keys for large tablesâœ“ Monitor query costs and optimize expensive queriesâœ“ Use appropriate data types (avoid SELECT *)âœ“ Consider materialized views for complex aggregationsâœ“ Use query tags for cost trackingâœ“ Implement incremental loading for large datasets\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Cost Optimization Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"\"\"1. Warehouse Management:   - Use auto-suspend (e.g., 5 minutes)   - Use auto-resume   - Right-size your warehouse2. Query Optimization:   - Use WHERE clauses to reduce data scanned   - Avoid SELECT * in production   - Use LIMIT for development/testing   - Leverage result caching3. Data Organization:   - Partition large tables by date   - Use clustering keys for frequently filtered columns   - Consider data retention policies4. Monitoring:   - Track query history and costs   - Identify expensive queries   - Set up cost alerts\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Incremental Loading Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"\"\"# Load only new data since last runlast_load_date = '2024-01-01'  # From metadata tablequery = f'''    SELECT *    FROM INSURANCE_DB.PUBLIC.CLAIMS    WHERE claim_date > '{last_load_date}'    AND claim_date <= CURRENT_DATE()'''new_data = load_from_snowflake(query)# Process new data# ...# Update metadata table with new last_load_dateupdate_query = f'''    UPDATE INSURANCE_DB.PUBLIC.ETL_METADATA    SET last_load_date = CURRENT_DATE()    WHERE table_name = 'claims''''\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"\"\"try:    df = load_from_snowflake(query, conn_params)except snowflake.connector.errors.ProgrammingError as e:    print(f\"Query error: {e}\")    # Handle SQL syntax errorsexcept snowflake.connector.errors.DatabaseError as e:    print(f\"Database error: {e}\")    # Handle connection issuesexcept Exception as e:    print(f\"Unexpected error: {e}\")    # Handle other errors\"\"\"\n",
    ")  # Step 10: Exerciseprint(\"\\nðŸŽ“ EXERCISE: Build a Snowflake Analytics Pipeline\")print(\"\"\"Create a script that:1. Connects to Snowflake2. Loads fraud predictions from the last 7 days3. Calculates lift and ROC metrics4. Saves results back to a Snowflake table5. Includes error handling and loggingBonus: Implement incremental loading and cost tracking\"\"\")print(\"\\n\" + \"=\" * 70)print(\"\\nKey Takeaways:\")print(\"1. Push computation to Snowflake when possible\")print(\"2. Use specific columns and filters to minimize data transfer\")print(\"3. Monitor and optimize query costs\")print(\"4. Implement incremental loading for efficiency\")print(\"5. Use proper error handling and connection management\")print(\"\\nNext: Tutorial 09 - End-to-End Pipeline\")print(\"\\nðŸ“– Additional Resources:\")print(\"   - Snowflake Python Connector docs\")print(\"   - Polars database integration guide\")print(\"   - Snowflake query optimization guide\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Try the exercise below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here",
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}