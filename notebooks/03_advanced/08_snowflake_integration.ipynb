{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 08: Snowflake Integration\n",
    "In this tutorial, you'll learn:\n",
    "- How to connect to Snowflake data warehouse\n",
    "- Loading large datasets efficiently from Snowflake\n",
    "- Best practices for cloud data warehouse integration\n",
    "- Performing analytics on Snowflake data\n",
    "Scenario:\n",
    "Your insurance company stores data in Snowflake. You need to load data\n",
    "for analysis while minimizing data transfer and compute costs.\n",
    "Prerequisites:\n",
    "- Snowflake account with appropriate permissions\n",
    "- Environment variables or config file with credentials\n",
    "- snowflake-connector-python package installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from analytics_store import model_validation, validation_plots\n",
    "from utils.database_helpers import get_snowflake_connection, load_from_snowflake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Checking Snowflake configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Snowflake configuration exists\n",
    "def check_snowflake_config():\n",
    "    required_vars = [\n",
    "        'SNOWFLAKE_ACCOUNT',\n",
    "        'SNOWFLAKE_USER',\n",
    "        'SNOWFLAKE_PASSWORD',\n",
    "        'SNOWFLAKE_WAREHOUSE'\n",
    "    ]\n",
    "    return all(os.getenv(var) for var in required_vars)\n",
    "\n",
    "if not check_snowflake_config():\n",
    "    print(\"\\nðŸ’¡ For this tutorial, you need:\")\n",
    "    print(\"   1. A Snowflake account\")\n",
    "    print(\"   2. Credentials set in environment variables\")\n",
    "    print(\"   3. Data loaded in Snowflake tables\")\n",
    "    print(\"\\nExample .env file:\")\n",
    "    print(\"\"\"\n",
    "    SNOWFLAKE_ACCOUNT=your_account.region\n",
    "    SNOWFLAKE_USER=your_username\n",
    "    SNOWFLAKE_PASSWORD=your_password\n",
    "    SNOWFLAKE_WAREHOUSE=your_warehouse\n",
    "    SNOWFLAKE_DATABASE=INSURANCE_DB\n",
    "    SNOWFLAKE_SCHEMA=PUBLIC\n",
    "    SNOWFLAKE_ROLE=ANALYST\n",
    "    \"\"\")\n",
    "    print(\"\\nSkipping Snowflake connection (demo mode)...\")\n",
    "    demo_mode = True\n",
    "else:\n",
    "    demo_mode = False\n",
    "    print(\"âœ“ Snowflake configuration found\")\n",
    "\n",
    "# Step 2: Connect to Snowflake\n",
    "if not demo_mode:\n",
    "    print(\"\\nðŸ”Œ Step 2: Connecting to Snowflake...\")\n",
    "    try:\n",
    "        conn_params = get_snowflake_connection()\n",
    "        print(f\"âœ“ Connecting to account: {conn_params['account']}\")\n",
    "        print(f\"âœ“ Using warehouse: {conn_params['warehouse']}\")\n",
    "        print(f\"âœ“ Database: {conn_params['database']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Connection error: {e}\")\n",
    "        demo_mode = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Efficient Data Loading Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nStrategy 1: Use SELECT with specific columns\")\n",
    "print(\"\"\"\n",
    "# Instead of SELECT *\n",
    "query = '''\n",
    "    SELECT\n",
    "        policy_id,\n",
    "        customer_age,\n",
    "        annual_premium,\n",
    "        policy_type\n",
    "    FROM INSURANCE_DB.PUBLIC.POLICIES\n",
    "    LIMIT 10000\n",
    "'''\n",
    "df = load_from_snowflake(query)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nStrategy 2: Filter data in Snowflake (not in Python)\")\n",
    "print(\"\"\"\n",
    "# Push filtering to Snowflake\n",
    "query = '''\n",
    "    SELECT *\n",
    "    FROM INSURANCE_DB.PUBLIC.CLAIMS\n",
    "    WHERE claim_date >= DATEADD(month, -6, CURRENT_DATE())\n",
    "    AND claim_amount > 10000\n",
    "    AND state IN ('CA', 'NY', 'TX')\n",
    "'''\n",
    "df = load_from_snowflake(query)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nStrategy 3: Use aggregations in Snowflake\")\n",
    "print(\"\"\"\n",
    "# Aggregate before loading\n",
    "query = '''\n",
    "    SELECT\n",
    "        policy_type,\n",
    "        state,\n",
    "        DATE_TRUNC('month', claim_date) as month,\n",
    "        COUNT(*) as claim_count,\n",
    "        AVG(claim_amount) as avg_claim,\n",
    "        SUM(CASE WHEN is_fraud = 1 THEN 1 ELSE 0 END) as fraud_count\n",
    "    FROM INSURANCE_DB.PUBLIC.CLAIMS\n",
    "    GROUP BY policy_type, state, month\n",
    "    ORDER BY month DESC\n",
    "'''\n",
    "df = load_from_snowflake(query)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Sample Queries for Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nQuery 1: Load fraud predictions for model evaluation\")\n",
    "fraud_query = \"\"\"\n",
    "SELECT\n",
    "    claim_id,\n",
    "    actual_fraud,\n",
    "    model1_fraud_score,\n",
    "    model2_fraud_score,\n",
    "    claim_amount,\n",
    "    policy_type\n",
    "FROM INSURANCE_DB.PUBLIC.FRAUD_PREDICTIONS\n",
    "WHERE prediction_date >= DATEADD(day, -30, CURRENT_DATE())\n",
    "\"\"\"\n",
    "print(fraud_query)\n",
    "\n",
    "print(\"\\nQuery 2: Load premium predictions with features\")\n",
    "premium_query = \"\"\"\n",
    "SELECT\n",
    "    policy_id,\n",
    "    customer_age,\n",
    "    credit_score,\n",
    "    coverage_amount,\n",
    "    actual_premium,\n",
    "    model1_predicted_premium,\n",
    "    model2_predicted_premium\n",
    "FROM INSURANCE_DB.PUBLIC.PREMIUM_PREDICTIONS\n",
    "WHERE prediction_date = CURRENT_DATE()\n",
    "\"\"\"\n",
    "print(premium_query)\n",
    "\n",
    "print(\"\\nQuery 3: Complex join for comprehensive analysis\")\n",
    "complex_query = \"\"\"\n",
    "SELECT\n",
    "    c.claim_id,\n",
    "    c.claim_amount,\n",
    "    c.claim_type,\n",
    "    c.is_fraud,\n",
    "    c.fraud_score,\n",
    "    p.policy_type,\n",
    "    p.customer_age,\n",
    "    p.state,\n",
    "    p.annual_premium,\n",
    "    p.coverage_amount,\n",
    "    p.credit_score\n",
    "FROM INSURANCE_DB.PUBLIC.CLAIMS c\n",
    "INNER JOIN INSURANCE_DB.PUBLIC.POLICIES p\n",
    "    ON c.policy_id = p.policy_id\n",
    "WHERE c.claim_date >= DATEADD(year, -1, CURRENT_DATE())\n",
    "AND c.claim_status = 'Approved'\n",
    "\"\"\"\n",
    "print(complex_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Demo Analysis (using local data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load local data to simulate Snowflake data\n",
    "data_path = project_root / 'data' / 'fraud_predictions.csv'\n",
    "\n",
    "if data_path.exists():\n",
    "    print(\"Loading sample data (simulating Snowflake query)...\")\n",
    "    df = pl.read_csv(data_path)\n",
    "    \n",
    "    # Perform analysis\n",
    "    print(f\"âœ“ Loaded {len(df)} records\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    lift_result = model_validation.calculate_lift_curve(\n",
    "        df,\n",
    "        target_column='actual_fraud',\n",
    "        score_column='model1_fraud_score',\n",
    "        n_bins=10\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nModel Performance:\")\n",
    "    print(f\"- AUC Lift: {lift_result.auc_score_lift:.4f}\")\n",
    "    print(f\"- Top decile lift: {lift_result.score_lift_values[0]:.2f}x\")\n",
    "    \n",
    "    # ROC analysis\n",
    "    roc_result = model_validation.calculate_roc_curve(\n",
    "        df,\n",
    "        target_column='actual_fraud',\n",
    "        score_column='model1_fraud_score'\n",
    "    )\n",
    "    \n",
    "    print(f\"- AUC Score: {roc_result.auc_score:.4f}\")\n",
    "    print(f\"- Optimal threshold: {roc_result.optimal_threshold:.4f}\")\n",
    "else:\n",
    "    print(\"âš  Sample data not found. Run: python utils/data_generators.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Snowflake Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "âœ“ Use warehouse size appropriate for your query\n",
    "âœ“ Filter and aggregate in Snowflake, not in Python\n",
    "âœ“ Use LIMIT for exploratory queries\n",
    "âœ“ Consider result caching for repeated queries\n",
    "âœ“ Use clustering keys for large tables\n",
    "âœ“ Monitor query costs and optimize expensive queries\n",
    "âœ“ Use appropriate data types (avoid SELECT *)\n",
    "âœ“ Consider materialized views for complex aggregations\n",
    "âœ“ Use query tags for cost tracking\n",
    "âœ“ Implement incremental loading for large datasets\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Cost Optimization Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "1. Warehouse Management:\n",
    "   - Use auto-suspend (e.g., 5 minutes)\n",
    "   - Use auto-resume\n",
    "   - Right-size your warehouse\n",
    "\n",
    "2. Query Optimization:\n",
    "   - Use WHERE clauses to reduce data scanned\n",
    "   - Avoid SELECT * in production\n",
    "   - Use LIMIT for development/testing\n",
    "   - Leverage result caching\n",
    "\n",
    "3. Data Organization:\n",
    "   - Partition large tables by date\n",
    "   - Use clustering keys for frequently filtered columns\n",
    "   - Consider data retention policies\n",
    "\n",
    "4. Monitoring:\n",
    "   - Track query history and costs\n",
    "   - Identify expensive queries\n",
    "   - Set up cost alerts\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Incremental Loading Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "# Load only new data since last run\n",
    "last_load_date = '2024-01-01'  # From metadata table\n",
    "\n",
    "query = f'''\n",
    "    SELECT *\n",
    "    FROM INSURANCE_DB.PUBLIC.CLAIMS\n",
    "    WHERE claim_date > '{last_load_date}'\n",
    "    AND claim_date <= CURRENT_DATE()\n",
    "'''\n",
    "\n",
    "new_data = load_from_snowflake(query)\n",
    "\n",
    "# Process new data\n",
    "# ...\n",
    "\n",
    "# Update metadata table with new last_load_date\n",
    "update_query = f'''\n",
    "    UPDATE INSURANCE_DB.PUBLIC.ETL_METADATA\n",
    "    SET last_load_date = CURRENT_DATE()\n",
    "    WHERE table_name = 'claims'\n",
    "'''\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "try:\n",
    "    df = load_from_snowflake(query, conn_params)\n",
    "except snowflake.connector.errors.ProgrammingError as e:\n",
    "    print(f\"Query error: {e}\")\n",
    "    # Handle SQL syntax errors\n",
    "except snowflake.connector.errors.DatabaseError as e:\n",
    "    print(f\"Database error: {e}\")\n",
    "    # Handle connection issues\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error: {e}\")\n",
    "    # Handle other errors\n",
    "\"\"\")\n",
    "\n",
    "# Step 10: Exercise\n",
    "print(\"\\nðŸŽ“ EXERCISE: Build a Snowflake Analytics Pipeline\")\n",
    "print(\"\"\"\n",
    "Create a script that:\n",
    "\n",
    "1. Connects to Snowflake\n",
    "2. Loads fraud predictions from the last 7 days\n",
    "3. Calculates lift and ROC metrics\n",
    "4. Saves results back to a Snowflake table\n",
    "5. Includes error handling and logging\n",
    "\n",
    "Bonus: Implement incremental loading and cost tracking\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"\\nKey Takeaways:\")\n",
    "print(\"1. Push computation to Snowflake when possible\")\n",
    "print(\"2. Use specific columns and filters to minimize data transfer\")\n",
    "print(\"3. Monitor and optimize query costs\")\n",
    "print(\"4. Implement incremental loading for efficiency\")\n",
    "print(\"5. Use proper error handling and connection management\")\n",
    "print(\"\\nNext: Tutorial 09 - End-to-End Pipeline\")\n",
    "\n",
    "print(\"\\nðŸ“– Additional Resources:\")\n",
    "print(\"   - Snowflake Python Connector docs\")\n",
    "print(\"   - Polars database integration guide\")\n",
    "print(\"   - Snowflake query optimization guide\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Try the exercise below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
