{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 02: ROC Curve Analysis\n",
    "In this tutorial, you'll learn:\n",
    "- What ROC curves are and how they differ from lift curves\n",
    "- How to calculate AUC (Area Under Curve) scores\n",
    "- How to find optimal classification thresholds\n",
    "- How to interpret sensitivity, specificity, and Youden's J statistic\n",
    "Scenario:\n",
    "You need to choose a threshold for your fraud detection model to decide\n",
    "which claims to investigate. ROC analysis helps you understand the trade-off\n",
    "between catching fraud (sensitivity) and avoiding false alarms (specificity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from analytics_store import model_validation, validation_plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Loading fraud prediction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = project_root / \"data\" / \"fraud_predictions.csv\"\n",
    "\n",
    "if not data_path.exists():\n",
    "    print(f\"[ERROR] Data file not found: {data_path}\")\n",
    "    print(\"Please run: python utils/data_generators.py\")\n",
    "\n",
    "df = pl.read_csv(data_path)\n",
    "print(f\"[OK] Loaded {len(df)} predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Calculating ROC curve for Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_result = model_validation.calculate_roc_curve(\n",
    "    df, target_column=\"actual_fraud\", score_column=\"model1_fraud_score\"\n",
    ")\n",
    "\n",
    "print(\"\\nROC Metrics:\")\n",
    "print(f\"- AUC Score: {roc_result.auc_score:.4f}\")\n",
    "print(f\"- Optimal Threshold: {roc_result.optimal_threshold:.4f}\")\n",
    "print(f\"- Number of threshold points: {len(roc_result.thresholds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Understanding the optimal threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nThe optimal threshold ({roc_result.optimal_threshold:.4f}) is found using\")\n",
    "print(\"Youden's J statistic, which maximizes (Sensitivity + Specificity - 1)\")\n",
    "print(\"\\nAt this threshold:\")\n",
    "\n",
    "# Find metrics at optimal threshold\n",
    "optimal_idx = roc_result.thresholds.index(roc_result.optimal_threshold)\n",
    "optimal_tpr = roc_result.tpr[optimal_idx]\n",
    "optimal_fpr = roc_result.fpr[optimal_idx]\n",
    "\n",
    "print(f\"- True Positive Rate (Sensitivity): {optimal_tpr:.2%}\")\n",
    "print(f\"- False Positive Rate: {optimal_fpr:.2%}\")\n",
    "print(f\"- Specificity: {(1 - optimal_fpr):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Interpreting AUC score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = roc_result.auc_score\n",
    "\n",
    "if auc >= 0.9:\n",
    "    interpretation = \"Excellent\"\n",
    "elif auc >= 0.8:\n",
    "    interpretation = \"Good\"\n",
    "elif auc >= 0.7:\n",
    "    interpretation = \"Fair\"\n",
    "elif auc >= 0.6:\n",
    "    interpretation = \"Poor\"\n",
    "else:\n",
    "    interpretation = \"Very Poor\"\n",
    "\n",
    "print(f\"\\nAUC Score: {auc:.4f} - {interpretation}\")\n",
    "print(\"\\nAUC Interpretation Guide:\")\n",
    "print(\"- 0.90-1.00: Excellent\")\n",
    "print(\"- 0.80-0.90: Good\")\n",
    "print(\"- 0.70-0.80: Fair\")\n",
    "print(\"- 0.60-0.70: Poor\")\n",
    "print(\"- 0.50-0.60: Very Poor (barely better than random)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Comparing different threshold strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conservative threshold (high specificity, low false alarms)\n",
    "conservative_threshold = 0.7\n",
    "conservative_idx = min(\n",
    "    range(len(roc_result.thresholds)),\n",
    "    key=lambda i: abs(roc_result.thresholds[i] - conservative_threshold),\n",
    ")\n",
    "\n",
    "# Aggressive threshold (high sensitivity, catch more fraud)\n",
    "aggressive_threshold = 0.3\n",
    "aggressive_idx = min(\n",
    "    range(len(roc_result.thresholds)),\n",
    "    key=lambda i: abs(roc_result.thresholds[i] - aggressive_threshold),\n",
    ")\n",
    "\n",
    "print(\"\\nThreshold Comparison:\")\n",
    "print(f\"\\n{'Strategy':<15} {'Threshold':<12} {'Sensitivity':<12} {'Specificity':<12}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "print(\n",
    "    f\"{'Conservative':<15} {conservative_threshold:<12.2f} \"\n",
    "    f\"{roc_result.tpr[conservative_idx]:<12.2%} \"\n",
    "    f\"{(1 - roc_result.fpr[conservative_idx]):<12.2%}\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"{'Optimal':<15} {roc_result.optimal_threshold:<12.4f} \"\n",
    "    f\"{optimal_tpr:<12.2%} {(1 - optimal_fpr):<12.2%}\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"{'Aggressive':<15} {aggressive_threshold:<12.2f} \"\n",
    "    f\"{roc_result.tpr[aggressive_idx]:<12.2%} \"\n",
    "    f\"{(1 - roc_result.fpr[aggressive_idx]):<12.2%}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Converting ROC results to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_df = roc_result.to_polars()\n",
    "print(\"\\nROC curve data (showing first 10 points):\")\n",
    "print(roc_df.head(10))\n",
    "\n",
    "# Save results\n",
    "output_dir = project_root / \"outputs\"\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "roc_df.write_csv(output_dir / \"02_roc_results.csv\")\n",
    "print(f\"\\n[OK] Results saved to: {output_dir / '02_roc_results.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Creating ROC curve visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    validation_plots.plot_roc_curve(\n",
    "        df,\n",
    "        target_column=\"actual_fraud\",\n",
    "        score_column=\"model1_fraud_score\",\n",
    "        title=\"Fraud Detection Model - ROC Curve\",\n",
    "    )\n",
    "    print(\"[OK] ROC curve plot displayed\")\n",
    "    print(\"(Close the plot window to continue)\")\n",
    "except Exception as e:\n",
    "    print(f\"[WARNING] Could not create plot: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Comparing all three models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Model 1\": \"model1_fraud_score\",\n",
    "    \"Model 2\": \"model2_fraud_score\",\n",
    "    \"Model 3\": \"model3_fraud_score\",\n",
    "}\n",
    "\n",
    "print(\"\\nModel Performance Comparison:\")\n",
    "print(f\"{'Model':<12} {'AUC':<10} {'Optimal Threshold':<20}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for model_name, score_col in models.items():\n",
    "    result = model_validation.calculate_roc_curve(\n",
    "        df, target_column=\"actual_fraud\", score_column=score_col\n",
    "    )\n",
    "    print(f\"{model_name:<12} {result.auc_score:<10.4f} {result.optimal_threshold:<20.4f}\")\n",
    "\n",
    "# Step 9: Exercise\n",
    "print(\"\\n[EXERCISE] EXERCISE: ROC with Confidence Intervals\")\n",
    "print(\"\\nTry calculating ROC curve with bootstrap confidence intervals:\")\n",
    "print(\n",
    "    \"\"\"\n",
    "roc_with_ci = model_validation.calculate_roc_curve(\n",
    "    df,\n",
    "    target_column='actual_fraud',\n",
    "    score_column='model1_fraud_score',\n",
    "    with_ci=True,\n",
    "    n_iterations=1000,\n",
    "    confidence_level=0.95\n",
    ")\n",
    "\n",
    "# Then visualize with:\n",
    "validation_plots.plot_roc_curve(\n",
    "    df,\n",
    "    target_column='actual_fraud',\n",
    "    score_column='model1_fraud_score',\n",
    "    with_ci=True,\n",
    "    n_iterations=1000\n",
    ")\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"\\nKey Takeaways:\")\n",
    "print(\"1. ROC curves show the trade-off between sensitivity and specificity\")\n",
    "print(\"2. AUC summarizes overall classification performance (0.5-1.0)\")\n",
    "print(\"3. Optimal threshold balances true positives and false positives\")\n",
    "print(\"4. Different thresholds suit different business needs\")\n",
    "print(\"\\nNext: Tutorial 03 - Regression Metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Try the exercise below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}