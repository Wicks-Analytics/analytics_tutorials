{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 03: Regression Metrics\n",
    "In this tutorial, you'll learn:\n",
    "- How to evaluate regression models for premium prediction\n",
    "- Understanding RMSE, MAE, and R-squared metrics\n",
    "- Creating diagnostic plots to identify model issues\n",
    "- Analyzing model performance by segments\n",
    "Scenario:\n",
    "You have built models to predict insurance premiums based on customer\n",
    "characteristics. You need to evaluate which model performs best and\n",
    "understand where the models might be making errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from analytics_store import model_validation, validation_plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Loading premium prediction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = project_root / \"data\" / \"premium_predictions.csv\"\n",
    "\n",
    "if not data_path.exists():\n",
    "    print(f\"âŒ Data file not found: {data_path}\")\n",
    "    print(\"Please run: python utils/data_generators.py\")\n",
    "\n",
    "df = pl.read_csv(data_path)\n",
    "print(f\"âœ“ Loaded {len(df)} predictions\")\n",
    "print(f\"\\nData preview:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Understanding the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Actual premium statistics:\")\n",
    "print(f\"- Mean: ${df['actual_premium'].mean():.2f}\")\n",
    "print(f\"- Median: ${df['actual_premium'].median():.2f}\")\n",
    "print(f\"- Std Dev: ${df['actual_premium'].std():.2f}\")\n",
    "print(f\"- Min: ${df['actual_premium'].min():.2f}\")\n",
    "print(f\"- Max: ${df['actual_premium'].max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Calculating regression metrics for Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = model_validation.calculate_regression_metrics(\n",
    "    df,\n",
    "    actual_column=\"actual_premium\",\n",
    "    predicted_column=\"model1_predicted_premium\",\n",
    "    n_features=4,  # We have 4 features: age, credit_score, prior_claims, coverage\n",
    ")\n",
    "\n",
    "print(f\"\\nRegression Metrics:\")\n",
    "print(f\"- RMSE (Root Mean Square Error): ${metrics.rmse:.2f}\")\n",
    "print(f\"- MAE (Mean Absolute Error): ${metrics.mae:.2f}\")\n",
    "print(f\"- R-squared: {metrics.r2:.4f}\")\n",
    "print(f\"- Adjusted R-squared: {metrics.adj_r2:.4f}\")\n",
    "print(f\"- Number of samples: {metrics.n_samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Interpreting the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nRMSE: ${metrics.rmse:.2f}\")\n",
    "print(f\"  â†’ On average, predictions are off by about ${metrics.rmse:.2f}\")\n",
    "print(f\"  â†’ As % of mean premium: {(metrics.rmse / df['actual_premium'].mean()) * 100:.1f}%\")\n",
    "\n",
    "print(f\"\\nMAE: ${metrics.mae:.2f}\")\n",
    "print(f\"  â†’ Median absolute error is ${metrics.mae:.2f}\")\n",
    "print(f\"  â†’ More robust to outliers than RMSE\")\n",
    "\n",
    "print(f\"\\nR-squared: {metrics.r2:.4f}\")\n",
    "print(f\"  â†’ Model explains {metrics.r2 * 100:.1f}% of variance in premiums\")\n",
    "\n",
    "if metrics.r2 >= 0.9:\n",
    "    interpretation = \"Excellent fit\"\n",
    "elif metrics.r2 >= 0.7:\n",
    "    interpretation = \"Good fit\"\n",
    "elif metrics.r2 >= 0.5:\n",
    "    interpretation = \"Moderate fit\"\n",
    "else:\n",
    "    interpretation = \"Poor fit\"\n",
    "\n",
    "print(f\"  â†’ {interpretation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Comparing all three models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Model 1\": \"model1_predicted_premium\",\n",
    "    \"Model 2\": \"model2_predicted_premium\",\n",
    "    \"Model 3\": \"model3_predicted_premium\",\n",
    "}\n",
    "\n",
    "print(\"\\nModel Performance Comparison:\")\n",
    "print(f\"{'Model':<12} {'RMSE':<12} {'MAE':<12} {'RÂ²':<10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "all_metrics = []\n",
    "for model_name, pred_col in models.items():\n",
    "    m = model_validation.calculate_regression_metrics(\n",
    "        df, actual_column=\"actual_premium\", predicted_column=pred_col, n_features=4\n",
    "    )\n",
    "    print(f\"{model_name:<12} ${m.rmse:<11.2f} ${m.mae:<11.2f} {m.r2:<10.4f}\")\n",
    "\n",
    "    # Store for later analysis\n",
    "    metrics_df = m.to_polars()\n",
    "    metrics_df = metrics_df.with_columns(pl.lit(model_name).alias(\"model_name\"))\n",
    "    all_metrics.append(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Saving combined metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_metrics = pl.concat(all_metrics)\n",
    "\n",
    "output_dir = project_root / \"outputs\"\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "combined_metrics.write_csv(output_dir / \"03_regression_metrics.csv\")\n",
    "\n",
    "print(f\"âœ“ Metrics saved to: {output_dir / '03_regression_metrics.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Creating diagnostic plots for Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    validation_plots.plot_regression_diagnostics(\n",
    "        df,\n",
    "        actual_column=\"actual_premium\",\n",
    "        predicted_column=\"model1_predicted_premium\",\n",
    "        title=\"Premium Prediction Model 1 - Diagnostics\",\n",
    "    )\n",
    "    print(\"âœ“ Diagnostic plots displayed\")\n",
    "    print(\"\\nThe diagnostic plots show:\")\n",
    "    print(\"  1. Actual vs Predicted: Should follow diagonal line\")\n",
    "    print(\"  2. Residual Plot: Should be randomly scattered around 0\")\n",
    "    print(\"  3. Q-Q Plot: Should follow diagonal if residuals are normal\")\n",
    "    print(\"(Close the plot window to continue)\")\n",
    "except Exception as e:\n",
    "    print(f\"âš  Could not create plot: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Analyzing errors by customer segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add age groups\n",
    "df_with_groups = df.with_columns(\n",
    "    [\n",
    "        pl.when(pl.col(\"customer_age\") < 30)\n",
    "        .then(pl.lit(\"Under 30\"))\n",
    "        .when(pl.col(\"customer_age\") < 50)\n",
    "        .then(pl.lit(\"30-49\"))\n",
    "        .when(pl.col(\"customer_age\") < 65)\n",
    "        .then(pl.lit(\"50-64\"))\n",
    "        .otherwise(pl.lit(\"65+\"))\n",
    "        .alias(\"age_group\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Calculate errors by group\n",
    "df_with_errors = df_with_groups.with_columns(\n",
    "    [\n",
    "        (pl.col(\"model1_predicted_premium\") - pl.col(\"actual_premium\")).alias(\"error\"),\n",
    "        ((pl.col(\"model1_predicted_premium\") - pl.col(\"actual_premium\")).abs()).alias(\"abs_error\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\nError Analysis by Age Group:\")\n",
    "print(f\"{'Age Group':<12} {'Count':<8} {'Mean Error':<15} {'Mean Abs Error':<15}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for age_group in [\"Under 30\", \"30-49\", \"50-64\", \"65+\"]:\n",
    "    group_df = df_with_errors.filter(pl.col(\"age_group\") == age_group)\n",
    "    count = len(group_df)\n",
    "    mean_error = group_df[\"error\"].mean()\n",
    "    mean_abs_error = group_df[\"abs_error\"].mean()\n",
    "    print(f\"{age_group:<12} {count:<8} ${mean_error:<14.2f} ${mean_abs_error:<14.2f}\")\n",
    "\n",
    "# Step 9: Exercise\n",
    "print(\"\\nðŸŽ“ EXERCISE: Analyze by Coverage Amount\")\n",
    "print(\"\\nTry analyzing model performance by coverage amount:\")\n",
    "print(\n",
    "    \"\"\"\n",
    "# Create coverage groups\n",
    "df_coverage = df.with_columns([\n",
    "    pl.when(pl.col('coverage_amount') < 50000).then(pl.lit('Low'))\n",
    "    .when(pl.col('coverage_amount') < 250000).then(pl.lit('Medium'))\n",
    "    .otherwise(pl.lit('High')).alias('coverage_group')\n",
    "])\n",
    "\n",
    "# Calculate metrics for each group\n",
    "for group in ['Low', 'Medium', 'High']:\n",
    "    group_df = df_coverage.filter(pl.col('coverage_group') == group)\n",
    "    metrics = model_validation.calculate_regression_metrics(\n",
    "        group_df,\n",
    "        actual_column='actual_premium',\n",
    "        predicted_column='model1_predicted_premium'\n",
    "    )\n",
    "    print(f\"{group}: RMSE=${metrics.rmse:.2f}, RÂ²={metrics.r2:.4f}\")\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"\\nKey Takeaways:\")\n",
    "print(\"1. RMSE and MAE measure prediction error in original units\")\n",
    "print(\"2. R-squared shows proportion of variance explained (0-1)\")\n",
    "print(\"3. Diagnostic plots help identify systematic errors\")\n",
    "print(\"4. Segment analysis reveals where models perform well/poorly\")\n",
    "print(\"\\nNext: Tutorial 04 - Model Comparison (Intermediate)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Try the exercise below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
