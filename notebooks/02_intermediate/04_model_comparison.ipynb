{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 04: Model Comparison with Double Lift\n",
    "In this tutorial, you'll learn:\n",
    "- How to compare two models using double lift analysis\n",
    "- Understanding joint lift and conditional lift metrics\n",
    "- Analyzing score correlation between models\n",
    "- Making informed decisions about model selection\n",
    "Scenario:\n",
    "You have multiple fraud detection models and need to understand:\n",
    "- Which model performs better\n",
    "- Whether models are capturing different patterns\n",
    "- If combining models could improve performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from analytics_store import model_validation, validation_plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Loading fraud prediction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = project_root / \"data\" / \"fraud_predictions.csv\"\n",
    "\n",
    "if not data_path.exists():\n",
    "    print(f\"[ERROR] Data file not found: {data_path}\")\n",
    "    print(\"Please run: python setup_database.py\")\n",
    "\n",
    "df = pl.read_csv(data_path)\n",
    "print(f\"[OK] Loaded {len(df)} predictions with 3 models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Evaluating individual model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Model 1\": \"model1_fraud_score\",\n",
    "    \"Model 2\": \"model2_fraud_score\",\n",
    "    \"Model 3\": \"model3_fraud_score\",\n",
    "}\n",
    "\n",
    "print(\"\\nIndividual Model Metrics:\")\n",
    "print(f\"{'Model':<12} {'AUC Lift':<12} {'AUC ROC':<12} {'Top Decile Lift':<15}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "individual_results = {}\n",
    "for model_name, score_col in models.items():\n",
    "    # Lift analysis\n",
    "    lift_result = model_validation.calculate_lift_curve(\n",
    "        df, target_column=\"actual_fraud\", score_column=score_col, n_bins=10\n",
    "    )\n",
    "\n",
    "    # ROC analysis\n",
    "    roc_result = model_validation.calculate_roc_curve(\n",
    "        df, target_column=\"actual_fraud\", score_column=score_col\n",
    "    )\n",
    "\n",
    "    individual_results[model_name] = {\"lift\": lift_result, \"roc\": roc_result}\n",
    "\n",
    "    print(\n",
    "        f\"{model_name:<12} {lift_result.auc_score_lift:<12.4f} \"\n",
    "        f\"{roc_result.auc_score:<12.4f} {lift_result.score_lift_values[0]:<15.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Comparing Model 1 vs Model 2 (Double Lift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_lift_result = model_validation.calculate_double_lift(\n",
    "    df,\n",
    "    target_column=\"actual_fraud\",\n",
    "    score1_column=\"model1_fraud_score\",\n",
    "    score2_column=\"model2_fraud_score\",\n",
    "    n_bins=10,\n",
    ")\n",
    "\n",
    "print(\"\\nDouble Lift Metrics:\")\n",
    "print(f\"- Score Correlation: {double_lift_result.correlation:.4f}\")\n",
    "print(f\"- Joint Lift: {double_lift_result.joint_lift:.4f}\")\n",
    "print(f\"- Conditional Lift: {double_lift_result.conditional_lift:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Interpreting score correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = double_lift_result.correlation\n",
    "print(f\"\\nCorrelation: {corr:.4f}\")\n",
    "\n",
    "if abs(corr) >= 0.9:\n",
    "    interpretation = \"Very high - models are highly similar\"\n",
    "elif abs(corr) >= 0.7:\n",
    "    interpretation = \"High - models capture similar patterns\"\n",
    "elif abs(corr) >= 0.5:\n",
    "    interpretation = \"Moderate - some overlap in patterns\"\n",
    "elif abs(corr) >= 0.3:\n",
    "    interpretation = \"Low - models capture different patterns\"\n",
    "else:\n",
    "    interpretation = \"Very low - models are largely independent\"\n",
    "\n",
    "print(f\"Interpretation: {interpretation}\")\n",
    "\n",
    "if abs(corr) < 0.7:\n",
    "    print(\"\\n[OK] Low correlation suggests models could be combined for better performance\")\n",
    "else:\n",
    "    print(\"\\n[WARNING] High correlation suggests models are redundant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Understanding joint and conditional lift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nJoint Lift: {double_lift_result.joint_lift:.4f}\")\n",
    "print(\"  -> Lift when BOTH models score high\")\n",
    "print(\"  -> Measures agreement between models\")\n",
    "\n",
    "print(f\"\\nConditional Lift: {double_lift_result.conditional_lift:.4f}\")\n",
    "print(\"  -> Lift of Model 2 when Model 1 scores high\")\n",
    "print(\"  -> Measures incremental value of Model 2\")\n",
    "\n",
    "if double_lift_result.conditional_lift > 1.2:\n",
    "    print(\"  [OK] Model 2 adds significant value beyond Model 1\")\n",
    "elif double_lift_result.conditional_lift > 1.0:\n",
    "    print(\"  ~ Model 2 adds some value beyond Model 1\")\n",
    "else:\n",
    "    print(\"  [X] Model 2 adds little value beyond Model 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Visualizing double lift comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    validation_plots.plot_double_lift(\n",
    "        df,\n",
    "        target_column=\"actual_fraud\",\n",
    "        score1_column=\"model1_fraud_score\",\n",
    "        score2_column=\"model2_fraud_score\",\n",
    "        score1_name=\"Model 1\",\n",
    "        score2_name=\"Model 2\",\n",
    "        title=\"Model Comparison: Model 1 vs Model 2\",\n",
    "    )\n",
    "    print(\"[OK] Double lift plot displayed\")\n",
    "    print(\"(Close the plot window to continue)\")\n",
    "except Exception as e:\n",
    "    print(f\"[WARNING] Could not create plot: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Comparing all model pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pairs = [\n",
    "    (\"Model 1\", \"model1_fraud_score\", \"Model 2\", \"model2_fraud_score\"),\n",
    "    (\"Model 1\", \"model1_fraud_score\", \"Model 3\", \"model3_fraud_score\"),\n",
    "    (\"Model 2\", \"model2_fraud_score\", \"Model 3\", \"model3_fraud_score\"),\n",
    "]\n",
    "\n",
    "print(\"\\nPairwise Comparison:\")\n",
    "print(f\"{'Pair':<20} {'Correlation':<15} {'Joint Lift':<15} {'Conditional Lift':<15}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for name1, col1, name2, col2 in model_pairs:\n",
    "    result = model_validation.calculate_double_lift(\n",
    "        df, target_column=\"actual_fraud\", score1_column=col1, score2_column=col2, n_bins=10\n",
    "    )\n",
    "    pair_name = f\"{name1} vs {name2}\"\n",
    "    print(\n",
    "        f\"{pair_name:<20} {result.correlation:<15.4f} \"\n",
    "        f\"{result.joint_lift:<15.4f} {result.conditional_lift:<15.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Model Selection Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best model by AUC\n",
    "best_model = max(individual_results.items(), key=lambda x: x[1][\"roc\"].auc_score)\n",
    "\n",
    "print(f\"\\nBest Individual Model: {best_model[0]}\")\n",
    "print(f\"- AUC: {best_model[1]['roc'].auc_score:.4f}\")\n",
    "print(f\"- Top Decile Lift: {best_model[1]['lift'].score_lift_values[0]:.2f}x\")\n",
    "\n",
    "# Check if models are complementary\n",
    "print(\"\\nModel Combination Potential:\")\n",
    "m1_m2_corr = model_validation.calculate_double_lift(\n",
    "    df, \"actual_fraud\", \"model1_fraud_score\", \"model2_fraud_score\"\n",
    ").correlation\n",
    "\n",
    "if abs(m1_m2_corr) < 0.7:\n",
    "    print(\"[OK] Models 1 and 2 are complementary - consider ensemble\")\n",
    "else:\n",
    "    print(\"[X] Models are too similar - use best individual model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Creating a simple ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple average ensemble\n",
    "df_ensemble = df.with_columns(\n",
    "    [((pl.col(\"model1_fraud_score\") + pl.col(\"model2_fraud_score\")) / 2).alias(\"ensemble_score\")]\n",
    ")\n",
    "\n",
    "# Evaluate ensemble\n",
    "ensemble_lift = model_validation.calculate_lift_curve(\n",
    "    df_ensemble, target_column=\"actual_fraud\", score_column=\"ensemble_score\", n_bins=10\n",
    ")\n",
    "\n",
    "ensemble_roc = model_validation.calculate_roc_curve(\n",
    "    df_ensemble, target_column=\"actual_fraud\", score_column=\"ensemble_score\"\n",
    ")\n",
    "\n",
    "print(\"\\nEnsemble Performance:\")\n",
    "print(f\"- AUC: {ensemble_roc.auc_score:.4f}\")\n",
    "print(f\"- Top Decile Lift: {ensemble_lift.score_lift_values[0]:.2f}x\")\n",
    "\n",
    "# Compare with best individual\n",
    "improvement = ensemble_roc.auc_score - best_model[1][\"roc\"].auc_score\n",
    "print(f\"\\nImprovement over best individual: {improvement:+.4f}\")\n",
    "\n",
    "if improvement > 0.01:\n",
    "    print(\"[OK] Ensemble shows meaningful improvement\")\n",
    "else:\n",
    "    print(\"~ Ensemble shows marginal improvement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Saving comparison results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = project_root / \"outputs\"\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save double lift results\n",
    "double_lift_df = double_lift_result.to_polars()\n",
    "double_lift_df.write_csv(output_dir / \"04_double_lift_results.csv\")\n",
    "print(f\"[OK] Results saved to: {output_dir / '04_double_lift_results.csv'}\")\n",
    "\n",
    "# Step 11: Exercise\n",
    "print(\"\\n[EXERCISE] EXERCISE: Weighted Ensemble\")\n",
    "print(\"\\nTry creating a weighted ensemble based on individual model performance:\")\n",
    "print(\n",
    "    \"\"\"\n",
    "# Weight models by their AUC scores\n",
    "auc1 = individual_results['Model 1']['roc'].auc_score\n",
    "auc2 = individual_results['Model 2']['roc'].auc_score\n",
    "total_auc = auc1 + auc2\n",
    "\n",
    "weight1 = auc1 / total_auc\n",
    "weight2 = auc2 / total_auc\n",
    "\n",
    "df_weighted = df.with_columns([\n",
    "    (pl.col('model1_fraud_score') * weight1 +\n",
    "     pl.col('model2_fraud_score') * weight2)\n",
    "    .alias('weighted_ensemble')\n",
    "])\n",
    "\n",
    "# Evaluate weighted ensemble\n",
    "# Compare with simple average ensemble\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"\\nKey Takeaways:\")\n",
    "print(\"1. Double lift analysis reveals model complementarity\")\n",
    "print(\"2. Low correlation suggests models capture different patterns\")\n",
    "print(\"3. Joint lift measures agreement between models\")\n",
    "print(\"4. Conditional lift shows incremental value\")\n",
    "print(\"5. Ensemble models can outperform individual models\")\n",
    "print(\"\\nNext: Tutorial 05 - SQL Integration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Try the exercise below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}